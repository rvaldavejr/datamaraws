{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6653dabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8764c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "# Folder containing your Cluster Subfolders (e.g. /0001/dhs_0001_2023_Q1.tif)\n",
    "tif_folder = \"/Users/ruben/Desktop/Thesis/TrainingData/Sentinel2/sample50-quarterly-2022\"\n",
    "labels_file = \"viirs_ntl_labels_sample50.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ce412a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. CUSTOM TIF GENERATOR\n",
    "# ==========================================\n",
    "class SentinelGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size=32, target_size=(224, 224), n_classes=3):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.target_size = target_size\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_paths[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        \n",
    "        images = []\n",
    "        for path in batch_x:\n",
    "            images.append(self.load_and_process_tif(path))\n",
    "            \n",
    "        return np.array(images), np.array(batch_y)\n",
    "    \n",
    "    def load_and_process_tif(self, path):\n",
    "        try:\n",
    "            with rasterio.open(path) as src:\n",
    "                # Read RGB bands (1, 2, 3)\n",
    "                r = src.read(1)\n",
    "                g = src.read(2)\n",
    "                b = src.read(3)\n",
    "                img = np.dstack((r, g, b))\n",
    "                \n",
    "                # Robust Normalization (The \"Percentile Stretch\" we discussed)\n",
    "                # This ensures the CNN sees a visible image, not a black square\n",
    "                p2, p98 = np.percentile(img, (2, 98))\n",
    "                if p98 == p2: p98 = 255 # Prevent divide by zero for flat images\n",
    "                img = np.clip((img - p2) / (p98 - p2), 0, 1)\n",
    "                \n",
    "                # Resize to 224x224 for VGG16\n",
    "                img = tf.image.resize(img, self.target_size).numpy()\n",
    "                return img\n",
    "        except:\n",
    "            # Return black image on error\n",
    "            return np.zeros((self.target_size[0], self.target_size[1], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4131370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching files to labels...\n",
      "Found 200 valid training images.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. PREPARE FILE LIST\n",
    "# ==========================================\n",
    "print(\"Matching files to labels...\")\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "# Create a dictionary for fast lookup: {ClusterID: NTL_Class}\n",
    "label_map = dict(zip(labels_df['DHSCLUST'], labels_df['NTL_Class']))\n",
    "\n",
    "all_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# Walk through folders to find every quarterly/monthly TIF\n",
    "for root, dirs, files in os.walk(tif_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\") and \"dhs_\" in file:\n",
    "            try:\n",
    "                # Extract Cluster ID from filename (e.g., dhs_0001_...)\n",
    "                parts = file.split('_') \n",
    "                cluster_id = int(parts[1]) # Assumes 'dhs' is part 0, '0001' is part 1\n",
    "                \n",
    "                if cluster_id in label_map:\n",
    "                    all_paths.append(os.path.join(root, file))\n",
    "                    all_labels.append(label_map[cluster_id])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "print(f\"Found {len(all_paths)} valid training images.\")\n",
    "\n",
    "# Split into Training and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_paths, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Generators\n",
    "train_gen = SentinelGenerator(X_train, y_train, batch_size=32, n_classes=3)\n",
    "val_gen = SentinelGenerator(X_val, y_val, batch_size=32, n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b3db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. BUILD VGG16 PROXY MODEL\n",
    "# ==========================================\n",
    "def build_proxy_model():\n",
    "    # Load VGG16 (Pre-trained on ImageNet)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Freeze the base layers (We only train the new head first)\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # The \"Feature Vector\" Layer (4096 size)\n",
    "    x = Dense(4096, activation='relu', name='feature_vector')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # The Classification Head (3 Classes: Dark, Dim, Bright)\n",
    "    predictions = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_proxy_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4cd4358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Proxy Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4s/step - accuracy: 0.3938 - loss: 2.5155 - val_accuracy: 0.3250 - val_loss: 2.8421\n",
      "Epoch 2/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 1.4129 - val_accuracy: 0.5500 - val_loss: 1.0175\n",
      "Epoch 3/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.6687 - loss: 0.8394 - val_accuracy: 0.5000 - val_loss: 1.3179\n",
      "Epoch 4/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.6812 - loss: 0.6700 - val_accuracy: 0.6000 - val_loss: 0.7732\n",
      "Epoch 5/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.8625 - loss: 0.3524 - val_accuracy: 0.5500 - val_loss: 1.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9062 - loss: 0.2814 - val_accuracy: 0.6250 - val_loss: 0.7236\n",
      "Epoch 7/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9062 - loss: 0.2540 - val_accuracy: 0.6250 - val_loss: 0.7968\n",
      "Epoch 8/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9125 - loss: 0.2615 - val_accuracy: 0.6500 - val_loss: 0.7227\n",
      "Epoch 9/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9438 - loss: 0.2085 - val_accuracy: 0.6250 - val_loss: 0.7387\n",
      "Epoch 10/10\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.9688 - loss: 0.1573 - val_accuracy: 0.6750 - val_loss: 0.7474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as 'cnn_viirs_proxy.h5'. Phase 1 Complete!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. TRAIN\n",
    "# ==========================================\n",
    "print(\"Starting Proxy Training...\")\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,  # 10 epochs is usually enough for proxy tasks\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"cnn_viirs_proxy.h5\")\n",
    "print(\"Model saved as 'cnn_viirs_proxy.h5'. Phase 1 Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
