import numpy as np
from tensorflow.keras import models

# 1. Define the Feature Extractor
# We strip off the final "Softmax" classification layer.
# We want the output of the dense layer before it (named 'cnn_features' in previous code).
# If you didn't name it, use index -2 (second to last layer).
layer_name = 'cnn_features' 
extractor = models.Model(inputs=cnn_model.input, outputs=cnn_model.get_layer(layer_name).output)

print("Extracting features from all 600 images...")

# 2. Extract Features
# Use 'X' (the full dataset of 600 images sorted by Cluster then Month)
# shape changes from (600, 32, 32, 1) -> (600, 32)
cnn_features = extractor.predict(X, verbose=1)

# 3. Reshape for LSTM: (Samples, TimeSteps, Features)
# We have 50 Clusters, 12 Months per cluster.
# The '32' comes from the number of neurons in your 'cnn_features' layer.
num_clusters = 50
time_steps = 12
num_features = cnn_features.shape[1] # Should be 32

# CRITICAL: This reshape only works if your X data was sorted!
# (Cluster1_Jan, Cluster1_Feb... Cluster1_Dec, Cluster2_Jan...)
X_lstm = cnn_features.reshape(num_clusters, time_steps, num_features)

print(f"Final LSTM Input Shape: {X_lstm.shape}")
# Expected: (50, 12, 32)

import pandas as pd

# 1. Load Survey Data
survey_df = pd.read_csv('dhs_wealth_index.csv') # Must have 'DHSCLUST' and 'WealthIndex'

# 2. Create a reference dataframe for your sorted X data
# 'cluster_tracker' is the list of IDs we built while loading the images
x_ref = pd.DataFrame({'DHSCLUST': cluster_tracker[::12]}) # Slice by 12 to get unique IDs in order

# 3. Merge to get Y in the correct order
merged_df = x_ref.merge(survey_df, on='DHSCLUST', how='left')

# 4. Extract target values
y_wealth = merged_df['WealthIndex'].values

print(f"Target Data Shape: {y_wealth.shape}") # Should be (50,)

from tensorflow.keras import layers, models, optimizers

def build_poverty_lstm(input_shape):
    model = models.Sequential()
    
    # LSTM Layer
    # 64 units to capture temporal patterns
    model.add(layers.LSTM(64, input_shape=input_shape, return_sequences=False))
    model.add(layers.Dropout(0.3)) # Fight overfitting
    
    # Dense Interpretation
    model.add(layers.Dense(32, activation='relu'))
    
    # Final Regression Output (1 number: Wealth Index)
    model.add(layers.Dense(1, activation='linear'))
    
    model.compile(optimizer=optimizers.Adam(learning_rate=0.001), 
                  loss='mse',       # Mean Squared Error (Standard for regression)
                  metrics=['mae'])  # Mean Absolute Error (Easier to explain)
    return model

# Train
lstm = build_poverty_lstm(input_shape=(12, 32))

history_lstm = lstm.fit(
    X_lstm, y_wealth,
    epochs=100,
    batch_size=8,
    validation_split=0.2, # Uses last 10 clusters for validation
    verbose=1
)