{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import rasterio\n",
    "from tensorflow.keras.models import load_model, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "# Path to your Sentinel Quarterly Images\n",
    "tif_folder = \"/Users/ruben/Desktop/Thesis/TrainingData/Sentinel2/sample50-quarterly-2022\"\n",
    "\n",
    "# Path to the model you just trained\n",
    "model_path = \"cnn_viirs_proxy.h5\"\n",
    "\n",
    "# Output CSV\n",
    "output_csv = \"dynamic_features_per_quarter.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20747b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. LOAD & MODIFY MODEL\n",
    "# ==========================================\n",
    "print(\"Loading Proxy Model...\")\n",
    "full_model = load_model(model_path)\n",
    "\n",
    "# Extract output from the 'feature_vector' layer we named earlier\n",
    "# This returns the 4096-dimensional vector instead of the class prediction\n",
    "feature_extractor = Model(inputs=full_model.input, \n",
    "                          outputs=full_model.get_layer('feature_vector').output)\n",
    "\n",
    "print(\"Model ready for feature extraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6537d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. IMAGE PRE-PROCESSING FUNCTION\n",
    "# ==========================================\n",
    "def process_sentinel_tif(path):\n",
    "    try:\n",
    "        with rasterio.open(path) as src:\n",
    "            # Read RGB bands (1, 2, 3)\n",
    "            r = src.read(1)\n",
    "            g = src.read(2)\n",
    "            b = src.read(3)\n",
    "            img = np.dstack((r, g, b))\n",
    "            \n",
    "            # Normalize (Same robust percentile stretch used in training)\n",
    "            p2, p98 = np.percentile(img, (2, 98))\n",
    "            if p98 == p2: p98 = 255\n",
    "            img = np.clip((img - p2) / (p98 - p2), 0, 1)\n",
    "            \n",
    "            # Resize to 224x224 (Model Input Size)\n",
    "            img = tf.image.resize(img, (224, 224)).numpy()\n",
    "            return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ea1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. EXTRACTION LOOP\n",
    "# ==========================================\n",
    "data_records = []\n",
    "\n",
    "print(\"Starting extraction (this may take a moment)...\")\n",
    "\n",
    "# Walk through all cluster folders\n",
    "for root, dirs, files in os.walk(tif_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".tif\") and \"dhs_\" in file:\n",
    "            # Parse Filename: dhs_0001_2022_Q1.tif\n",
    "            try:\n",
    "                parts = file.replace('.tif', '').split('_')\n",
    "                cluster_id = int(parts[1])\n",
    "                quarter = parts[3] # \"Q1\", \"Q2\", etc.\n",
    "                \n",
    "                # Pre-process\n",
    "                file_path = os.path.join(root, file)\n",
    "                img = process_sentinel_tif(file_path)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Add batch dimension (1, 224, 224, 3)\n",
    "                    img_batch = np.expand_dims(img, axis=0)\n",
    "                    \n",
    "                    # EXTRACT FEATURES\n",
    "                    features = feature_extractor.predict(img_batch, verbose=0)\n",
    "                    \n",
    "                    # Flatten to 1D array (4096 items)\n",
    "                    feat_vec = features.flatten()\n",
    "                    \n",
    "                    # Store Result\n",
    "                    record = {\n",
    "                        'ClusterID': cluster_id,\n",
    "                        'Quarter': quarter\n",
    "                    }\n",
    "                    # Add feat_0, feat_1 ... feat_4095\n",
    "                    for i, val in enumerate(feat_vec):\n",
    "                        record[f'CNN_{i}'] = val\n",
    "                        \n",
    "                    data_records.append(record)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 5. SAVE\n",
    "# ==========================================\n",
    "if len(data_records) > 0:\n",
    "    df = pd.DataFrame(data_records)\n",
    "    \n",
    "    # Sort for tidiness\n",
    "    df = df.sort_values(by=['ClusterID', 'Quarter'])\n",
    "    \n",
    "    # Save (Warning: This file will be large!)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"SUCCESS: Extracted features for {len(df)} images.\")\n",
    "    print(f\"Shape: {df.shape} (Rows, 4096+ Features)\")\n",
    "    print(f\"Saved to: {output_csv}\")\n",
    "else:\n",
    "    print(\"FAILED: No features extracted.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
